{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load pyspng. Defaulting to pillow image backend.\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "try:\n",
    "    import pyspng\n",
    "    PYSPNG_IMPORTED = True\n",
    "except ImportError:\n",
    "    PYSPNG_IMPORTED = False\n",
    "    print(\"Could not load pyspng. Defaulting to pillow image backend.\")\n",
    "    from PIL import Image\n",
    "\n",
    "\n",
    "class FDF256Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dirpath: str,\n",
    "                 load_keypoints: bool = False,\n",
    "                 img_transform: torch.nn.Module = None,\n",
    "                 load_masks: bool = False,\n",
    "                 mask_transform: torch.nn.Module = None,\n",
    "                 load_impath: bool = False):\n",
    "        dirpath = pathlib.Path(dirpath)\n",
    "        self.dirpath = dirpath\n",
    "        self.img_transform = img_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.load_masks = load_masks\n",
    "        self.load_keypoints = load_keypoints\n",
    "        self.load_impath = load_impath\n",
    "        assert self.dirpath.is_dir(),\\\n",
    "            f\"Did not find dataset at: {dirpath}\"\n",
    "        image_dir = self.dirpath.joinpath(\"images\")\n",
    "        self.image_paths = list(image_dir.glob(\"*.png\"))\n",
    "        assert len(self.image_paths) > 0,\\\n",
    "            f\"Did not find images in: {image_dir}\"\n",
    "        self.image_paths.sort(key=lambda x: int(x.stem))\n",
    "        self.landmarks = np.load(self.dirpath.joinpath(\"landmarks.npy\")).reshape(-1, 7, 2).astype(np.float32)\n",
    "        self.bounding_boxes = torch.from_numpy(np.load(self.dirpath.joinpath(\"bounding_box.npy\")))\n",
    "        assert len(self.image_paths) == len(self.bounding_boxes)\n",
    "        assert len(self.image_paths) == len(self.landmarks)\n",
    "        print(\n",
    "            f\"Dataset loaded from: {dirpath}. Number of samples:{len(self)}\")\n",
    "\n",
    "    def get_mask(self, idx):\n",
    "        mask = torch.ones((1, 256, 256), dtype=torch.bool)\n",
    "        bounding_box = self.bounding_boxes[idx]\n",
    "        x0, y0, x1, y1 = bounding_box\n",
    "        mask[:, y0:y1, x0:x1] = 0\n",
    "        return mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        impath = self.image_paths[index]\n",
    "        print(impath)\n",
    "        if PYSPNG_IMPORTED:\n",
    "            with open(impath, \"rb\") as fp:\n",
    "                im = pyspng.load(fp.read())\n",
    "        else:\n",
    "            with Image.open(impath) as fp:\n",
    "                im = np.array(fp)\n",
    "        if self.img_transform is not None:\n",
    "            im = self.img_transform(im)\n",
    "        masks = self.get_mask(index)\n",
    "        if self.mask_transform is not None:\n",
    "            masks = self.mask_transform(masks)\n",
    "        landmark = self.landmarks[index]\n",
    "        batch = {\n",
    "            \"img\": im,\n",
    "        }\n",
    "        if self.load_masks:\n",
    "            batch[\"mask\"] = masks\n",
    "        if self.load_keypoints:\n",
    "            batch[\"keypoints\"] = landmark\n",
    "        if self.load_impath:\n",
    "            batch['impath'] = str(impath)\n",
    "            batch[\"face_bbox\"] = self.bounding_boxes[index]\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# from https://github.com/gitshanks/fer2013\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "json_file = open('/Users/balazsmorvay/Downloads/fer2013-master/fer.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"/Users/balazsmorvay/Downloads/fer2013-master/fer.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "#setting image resizing parameters\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "x=None\n",
    "y=None\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded from: /Users/balazsmorvay/Downloads/FDF256/train. Number of samples:241982\n",
      "Dataset loaded from: /Users/balazsmorvay/Downloads/FDF256/val. Number of samples:6531\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/0.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Emotion: Surprise\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/1.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Angry\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/2.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/3.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/4.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Sad\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/5.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Sad\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/6.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/7.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/8.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/9.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Emotion: Neutral\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/10.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/11.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Emotion: Fear\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/12.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/13.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/14.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Emotion: Happy\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/15.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/16.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/17.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/18.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/19.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/20.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/21.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/22.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Fear\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/23.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Emotion: Surprise\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/24.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/25.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/26.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/27.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/28.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/29.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Sad\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/30.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Neutral\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/31.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/32.png\n",
      "(256, 256, 3)\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Emotion: Sad\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/33.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/34.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/35.png\n",
      "(256, 256, 3)\n",
      "/Users/balazsmorvay/Downloads/FDF256/train/images/36.png\n",
      "(256, 256, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/balazsmorvay/PycharmProjects/facediffusion/annotate_emotions.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/balazsmorvay/PycharmProjects/facediffusion/annotate_emotions.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m cv2\u001b[39m.\u001b[39mrectangle(full_size_image, (x, y), (x \u001b[39m+\u001b[39m w, y \u001b[39m+\u001b[39m h), (\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m, \u001b[39m0\u001b[39m), \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/balazsmorvay/PycharmProjects/facediffusion/annotate_emotions.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#predicting the emotion\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/balazsmorvay/PycharmProjects/facediffusion/annotate_emotions.ipynb#W1sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m yhat\u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39;49mpredict(cropped_img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/balazsmorvay/PycharmProjects/facediffusion/annotate_emotions.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEmotion: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mlabels[\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39margmax(yhat))])\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/engine/training.py:2647\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_predict_function()\n\u001b[1;32m   2646\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_counter\u001b[39m.\u001b[39massign(\u001b[39m0\u001b[39m)\n\u001b[0;32m-> 2647\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_predict_begin()\n\u001b[1;32m   2648\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautotune_steps_per_execution:\n\u001b[1;32m   2649\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_tuner\u001b[39m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/callbacks.py:574\u001b[0m, in \u001b[0;36mCallbackList.on_predict_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    572\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    573\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 574\u001b[0m     callback\u001b[39m.\u001b[39;49mon_predict_begin(logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/callbacks.py:1084\u001b[0m, in \u001b[0;36mProgbarLogger.on_predict_begin\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_predict_begin\u001b[39m(\u001b[39mself\u001b[39m, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1083\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset_progbar()\n\u001b[0;32m-> 1084\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_init_progbar()\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/callbacks.py:1134\u001b[0m, in \u001b[0;36mProgbarLogger._maybe_init_progbar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstateful_metrics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstateful_metrics\u001b[39m.\u001b[39munion(\n\u001b[1;32m   1130\u001b[0m         \u001b[39mset\u001b[39m(m\u001b[39m.\u001b[39mname \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmetrics)\n\u001b[1;32m   1131\u001b[0m     )\n\u001b[1;32m   1133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar \u001b[39m=\u001b[39m Progbar(\n\u001b[1;32m   1135\u001b[0m         target\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget,\n\u001b[1;32m   1136\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m   1137\u001b[0m         stateful_metrics\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstateful_metrics,\n\u001b[1;32m   1138\u001b[0m         unit_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mstep\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muse_steps \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39msample\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39m_update_stateful_metrics(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstateful_metrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/facediffusion/lib/python3.11/site-packages/keras/src/utils/generic_utils.py:143\u001b[0m, in \u001b[0;36mProgbar.__init__\u001b[0;34m(self, target, width, verbose, interval, stateful_metrics, unit_name)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mkeras.utils.Progbar\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mProgbar\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Displays a progress bar.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m        unit_name: Display name for step counts (usually \"step\" or \"sample\").\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    144\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    145\u001b[0m         target,\n\u001b[1;32m    146\u001b[0m         width\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,\n\u001b[1;32m    147\u001b[0m         verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    148\u001b[0m         interval\u001b[39m=\u001b[39m\u001b[39m0.05\u001b[39m,\n\u001b[1;32m    149\u001b[0m         stateful_metrics\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    150\u001b[0m         unit_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    151\u001b[0m     ):\n\u001b[1;32m    152\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget \u001b[39m=\u001b[39m target\n\u001b[1;32m    153\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m width\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n",
    "\n",
    "train_dataset = FDF256Dataset(dirpath='/Users/balazsmorvay/Downloads/FDF256/train/', load_keypoints=True, load_masks=True, load_impath=False)\n",
    "val_dataset = FDF256Dataset(dirpath='/Users/balazsmorvay/Downloads/FDF256/val/', load_keypoints=True, load_masks=True, load_impath=False)\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    full_size_image = np.array(batch['img'].squeeze())\n",
    "    print(full_size_image.shape)\n",
    "    gray=cv2.cvtColor(full_size_image, cv2.COLOR_RGB2GRAY)\n",
    "    face = cv2.CascadeClassifier('/Users/balazsmorvay/Downloads/fer2013-master/haarcascade_frontalface_default.xml')\n",
    "    faces = face.detectMultiScale(gray, 1.3  , 10)\n",
    "    \n",
    "    #detecting faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "        cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "        cv2.rectangle(full_size_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "        #predicting the emotion\n",
    "        yhat= loaded_model.predict(cropped_img)\n",
    "        print(\"Emotion: \"+labels[int(np.argmax(yhat))])\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
